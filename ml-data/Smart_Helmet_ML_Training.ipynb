{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Smart Helmet Accident Detection - Model Training\n",
                "\n",
                "## Setup Instructions:\n",
                "1. Upload this notebook to Google Colab\n",
                "2. Upload your `collected_data` folder (see next cell)\n",
                "3. Run all cells\n",
                "4. Download the generated files at the end\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Upload Your Data\n",
                "\n",
                "**Instructions:**\n",
                "1. In the left sidebar, click the folder icon (Files)\n",
                "2. Create a new folder called `collected_data`\n",
                "3. Inside it, create two folders: `normal` and `accident`\n",
                "4. Upload your CSV files:\n",
                "   - 3 files to `normal/`\n",
                "   - 3 files to `accident/`\n",
                "\n",
                "**OR** run this cell to upload via dialog:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Create directories\n",
                "os.makedirs('collected_data/normal', exist_ok=True)\n",
                "os.makedirs('collected_data/accident', exist_ok=True)\n",
                "\n",
                "print(\"Upload NORMAL CSV files:\")\n",
                "uploaded = files.upload()\n",
                "for filename in uploaded.keys():\n",
                "    os.rename(filename, f'collected_data/normal/{filename}')\n",
                "\n",
                "print(\"\\nUpload ACCIDENT CSV files:\")\n",
                "uploaded = files.upload()\n",
                "for filename in uploaded.keys():\n",
                "    os.rename(filename, f'collected_data/accident/{filename}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "WINDOW_SIZE = 100  # 2 seconds at 50Hz\n",
                "OVERLAP = 50       # 50% overlap\n",
                "FEATURES = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n",
                "LABELS = {'normal': 0, 'accident': 1}\n",
                "\n",
                "def load_data(data_dir='collected_data'):\n",
                "    \"\"\"Load all CSV files.\"\"\"\n",
                "    all_data = []\n",
                "    \n",
                "    for label_name, label_value in LABELS.items():\n",
                "        label_dir = os.path.join(data_dir, label_name)\n",
                "        csv_files = [f for f in os.listdir(label_dir) if f.endswith('.csv')]\n",
                "        print(f\"\\nLoading {label_name}: {len(csv_files)} files\")\n",
                "        \n",
                "        for csv_file in csv_files:\n",
                "            file_path = os.path.join(label_dir, csv_file)\n",
                "            df = pd.read_csv(file_path)\n",
                "            df['label'] = label_value\n",
                "            all_data.append(df)\n",
                "            print(f\"  ✓ {csv_file}: {len(df)} samples\")\n",
                "    \n",
                "    return pd.concat(all_data, ignore_index=True)\n",
                "\n",
                "# Load data\n",
                "df = load_data()\n",
                "print(f\"\\n✓ Total samples: {len(df)}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create Time Windows"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_windows(df, window_size=WINDOW_SIZE, overlap=OVERLAP):\n",
                "    \"\"\"Create sliding windows.\"\"\"\n",
                "    windows = []\n",
                "    labels = []\n",
                "    step = window_size - overlap\n",
                "    \n",
                "    for label in df['label'].unique():\n",
                "        label_data = df[df['label'] == label][FEATURES].values\n",
                "        \n",
                "        for i in range(0, len(label_data) - window_size + 1, step):\n",
                "            window = label_data[i:i + window_size]\n",
                "            if len(window) == window_size:\n",
                "                windows.append(window)\n",
                "                labels.append(label)\n",
                "    \n",
                "    return np.array(windows), np.array(labels)\n",
                "\n",
                "X, y = create_windows(df)\n",
                "print(f\"✓ Created {len(X)} windows\")\n",
                "print(f\"  Shape: {X.shape}\")\n",
                "print(f\"  Normal: {np.sum(y == 0)}\")\n",
                "print(f\"  Accident: {np.sum(y == 1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Normalize Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize\n",
                "n_samples, n_timesteps, n_features = X.shape\n",
                "X_reshaped = X.reshape(-1, n_features)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_normalized = scaler.fit_transform(X_reshaped)\n",
                "X_normalized = X_normalized.reshape(n_samples, n_timesteps, n_features)\n",
                "\n",
                "print(f\"✓ Data normalized\")\n",
                "print(f\"  Mean: {X_normalized.mean():.4f}\")\n",
                "print(f\"  Std: {X_normalized.std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Train/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"✓ Split complete\")\n",
                "print(f\"  Training: {len(X_train)} windows\")\n",
                "print(f\"  Testing: {len(X_test)} windows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Build Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = keras.Sequential([\n",
                "    layers.Input(shape=(WINDOW_SIZE, len(FEATURES))),\n",
                "    layers.Conv1D(16, kernel_size=3, activation='relu'),\n",
                "    layers.MaxPooling1D(pool_size=2),\n",
                "    layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
                "    layers.GlobalAveragePooling1D(),\n",
                "    layers.Dense(16, activation='relu'),\n",
                "    layers.Dropout(0.3),\n",
                "    layers.Dense(1, activation='sigmoid')\n",
                "])\n",
                "\n",
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = model.fit(\n",
                "    X_train, y_train,\n",
                "    epochs=50,\n",
                "    batch_size=32,\n",
                "    validation_split=0.2,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Plot training history\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['accuracy'], label='Train')\n",
                "plt.plot(history.history['val_accuracy'], label='Validation')\n",
                "plt.title('Model Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['loss'], label='Train')\n",
                "plt.plot(history.history['val_loss'], label='Validation')\n",
                "plt.title('Model Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(f\"\\n✓ Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
                "print(f\"✓ Test Loss: {test_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Convert to TensorFlow Lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to TFLite\n",
                "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "# Save TFLite model\n",
                "with open('accident_model.tflite', 'wb') as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "print(f\"✓ TFLite model size: {len(tflite_model) / 1024:.2f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Generate C Header File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to C array\n",
                "hex_array = ', '.join([f'0x{byte:02x}' for byte in tflite_model])\n",
                "\n",
                "header_content = f\"\"\"// Auto-generated TensorFlow Lite model\n",
                "// Smart Helmet Accident Detection Model\n",
                "\n",
                "#ifndef ACCIDENT_MODEL_H\n",
                "#define ACCIDENT_MODEL_H\n",
                "\n",
                "const unsigned int accident_model_len = {len(tflite_model)};\n",
                "const unsigned char accident_model[] = {{\n",
                "  {hex_array}\n",
                "}};\n",
                "\n",
                "#endif  // ACCIDENT_MODEL_H\n",
                "\"\"\"\n",
                "\n",
                "with open('accident_model.h', 'w') as f:\n",
                "    f.write(header_content)\n",
                "\n",
                "print(\"✓ C header file generated\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 12: Save Scaler Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "scaler_params = {\n",
                "    'mean': scaler.mean_.tolist(),\n",
                "    'scale': scaler.scale_.tolist()\n",
                "}\n",
                "\n",
                "with open('scaler_params.json', 'w') as f:\n",
                "    json.dump(scaler_params, f, indent=2)\n",
                "\n",
                "print(\"✓ Scaler parameters saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 13: Download Files\n",
                "\n",
                "Run this cell to download all generated files:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"Downloading files...\")\n",
                "files.download('accident_model.tflite')\n",
                "files.download('accident_model.h')\n",
                "files.download('scaler_params.json')\n",
                "\n",
                "print(\"\\n✅ ALL DONE!\")\n",
                "print(f\"\\nModel Performance: {test_accuracy * 100:.2f}% accuracy\")\n",
                "print(f\"Model Size: {len(tflite_model) / 1024:.2f} KB\")\n",
                "print(\"\\nNext: Copy accident_model.h to your ESP32 project!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}